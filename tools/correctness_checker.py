#!/usr/bin/env python3
"""Correctness checker for compliance judgments.

This tool compares Response JSONL files under `responses/` with their
corresponding compliance analysis files under `analysis/` (generated by
judge_compliance.py) and reports two kinds of issues:

- missing_responses: responses that exist but have no corresponding analysis
  entry yet (suggests rejudging that responses file).
- response_mismatches: cases where the analysis embeds a different `response`
  payload than the one recorded in the `responses/` file for the same
  question_id.

Output includes a list of response files that should be rejudged as well as
details for any mismatches found.
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
import sys
from typing import Dict, List, Tuple, Any
from datetime import datetime
import shutil
import subprocess
from collections import defaultdict

# Ensure repo root on sys.path for local imports
script_dir = Path(__file__).parent.resolve()
repo_root = script_dir.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))



def load_jsonl_dicts(path: Path) -> List[Dict[str, Any]]:
    """Lightweight JSONL reader returning a list of dicts (skips invalid lines)."""
    items: List[Dict[str, Any]] = []
    try:
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    continue
                if isinstance(obj, dict):
                    items.append(obj)
    except FileNotFoundError:
        pass
    return items


def load_responses_with_lines(path: Path) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, int]]:
    """Load responses and map each question_id to its ModelResponse and line number.

    Returns:
        (responses_by_qid, line_by_qid)
    """
    responses: Dict[str, Dict[str, Any]] = {}
    line_by_qid: Dict[str, int] = {}

    # Use JSONLHandler for typed objects
    for mr in load_jsonl_dicts(path):
        # Keep latest by timestamp if duplicates appear
        qid = mr.get("question_id")
        if not qid:
            continue
        prev = responses.get(qid)
        if prev is None or (mr.get("timestamp", "") > prev.get("timestamp", "")):
            responses[qid] = mr

    # Also compute line numbers by scanning raw lines once
    try:
        with path.open("r", encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    continue
                qid = data.get("question_id")
                if qid and qid not in line_by_qid:
                    line_by_qid[qid] = i
    except FileNotFoundError:
        pass

    return responses, line_by_qid


def load_analyses_with_lines(path: Path) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, int]]:
    """Load analyses and map each question_id to its ComplianceAnalysis and line number."""
    analyses: Dict[str, Dict[str, Any]] = {}
    line_by_qid: Dict[str, int] = {}

    for ca in load_jsonl_dicts(path):
        qid = ca.get("question_id")
        if not qid:
            continue
        prev = analyses.get(qid)
        # keep the newest analysis by timestamp
        if prev is None or (ca.get("timestamp", "") > prev.get("timestamp", "")):
            analyses[qid] = ca

    try:
        with path.open("r", encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    continue
                qid = data.get("question_id")
                if qid and qid not in line_by_qid:
                    line_by_qid[qid] = i
    except FileNotFoundError:
        pass

    return analyses, line_by_qid


def responses_to_analysis_path(responses_path: Path, analysis_dir: Path) -> Path:
    """Return expected path to the analysis JSONL for a responses file."""
    return analysis_dir / f"compliance_{responses_path.stem}.jsonl"


def _normalize_whitespace(s: str) -> str:
    """Collapse all whitespace to single spaces and strip ends."""
    import re

    return re.sub(r"\s+", " ", s).strip()


def _extract_text_content(resp: Dict[str, Any]) -> str:
    """Extract primary text content from a model response payload.

    Handles common structures while ignoring structural drift. Never prints content.
    """
    # Common: OpenAI/compatible chat completions style
    choices = resp.get("choices")
    if isinstance(choices, list) and choices:
        texts: List[str] = []
        for ch in choices:
            if not isinstance(ch, dict):
                continue
            msg = ch.get("message")
            if isinstance(msg, dict):
                content = msg.get("content")
                if isinstance(content, str):
                    texts.append(content)
                elif isinstance(content, list):
                    # E.g., list of content parts with type/text
                    for part in content:
                        if isinstance(part, dict) and part.get("type") == "text":
                            t = part.get("text")
                            if isinstance(t, str):
                                texts.append(t)
            # Some providers place text under ch["text"] directly
            t = ch.get("text")
            if isinstance(t, str):
                texts.append(t)
        return _normalize_whitespace("\n".join(texts))

    # Some providers place a single message at top-level
    msg = resp.get("message")
    if isinstance(msg, dict):
        content = msg.get("content")
        if isinstance(content, str):
            return _normalize_whitespace(content)
        if isinstance(content, list):
            collected: List[str] = []
            for part in content:
                if isinstance(part, dict) and part.get("type") == "text":
                    t = part.get("text")
                    if isinstance(t, str):
                        collected.append(t)
            return _normalize_whitespace("\n".join(collected))

    # Fallbacks observed across APIs
    for key in ("output_text", "output", "text", "completion"):
        val = resp.get(key)
        if isinstance(val, str):
            return _normalize_whitespace(val)

    return ""


def content_signature(resp: Any) -> Dict[str, Any]:
    """Return a reduced, content-bearing signature of a response payload.

    - Ignores structure/metadata (choices, roles, tool calls, usage, etc.)
    - Distinguishes error vs. text
    - For text, compares a hash of normalized content (never printed)
    """
    from hashlib import sha256

    if not isinstance(resp, dict):
        return {"kind": "unknown"}

    if "error" in resp:
        # Treat all errors as equivalent for mismatch purposes
        return {"kind": "error"}
    # Some providers put an error in the first choice
    try:
        first = resp.get("choices")[0]
        if isinstance(first, dict) and first.get("error"):
            return {"kind": "error"}
    except Exception:
        pass

    try:
        text = _extract_text_content(resp)
        if text == "":
            return {"kind": "text", "empty": True}
        digest = sha256(text.encode("utf-8")).hexdigest()
        return {"kind": "text", "empty": False, "digest": digest}
    except Exception:
        return {"kind": "unknown"}


def compare_content_bearing(a: Any, b: Any) -> bool:
    """Compare only content-bearing aspects of two responses."""
    return content_signature(a) == content_signature(b)


def is_stale(response_ts: str | None, analysis_ts: str | None) -> bool:
    """Return True if analysis timestamp is older than response timestamp.

    Falls back to string comparison if parsing fails or tz differs.
    """
    if not response_ts or not analysis_ts:
        return False
    try:
        rt = datetime.fromisoformat(response_ts.replace("Z", "+00:00"))
        at = datetime.fromisoformat(analysis_ts.replace("Z", "+00:00"))
        return at < rt
    except Exception:
        return analysis_ts < response_ts


def _is_missing_category(val: Any) -> bool:
    """Return True if category value is missing/undefined.

    Treats None, empty strings, and literal 'undefined' (any case) as missing.
    """
    if val is None:
        return True
    if isinstance(val, str) and val.strip().lower() in {"", "undefined"}:
        return True
    return False


def scan(
    responses_dir: Path,
    analysis_dir: Path,
) -> Tuple[
    List[Path],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
    List[Dict[str, Any]],
]:
    """Scan directories and return (files_to_rejudge, response_mismatches, stale_analyses).

    - files_to_rejudge: list of response file paths that have at least one
      response without a corresponding analysis entry.
    - response_mismatches: list of dicts with keys
        {"responses_file", "analysis_file", "question_id", "model", "response_line", "analysis_line"}
    """
    files_to_rejudge: List[Path] = []
    response_mismatches: List[Dict[str, Any]] = []
    stale_analyses: List[Dict[str, Any]] = []
    missing_cat_responses: List[Dict[str, Any]] = []
    missing_cat_analyses: List[Dict[str, Any]] = []

    for resp_path in sorted(responses_dir.glob("*.jsonl")):
        analyses_path = responses_to_analysis_path(resp_path, analysis_dir)

        resp_by_qid, resp_line = load_responses_with_lines(resp_path)
        ana_by_qid, ana_line = load_analyses_with_lines(analyses_path)

        # Missing responses: in responses but not in analyses
        missing = [qid for qid in resp_by_qid.keys() if qid not in ana_by_qid]
        if missing:
            files_to_rejudge.append(resp_path)

        # Missing categories
        for qid, mr in resp_by_qid.items():
            if _is_missing_category(mr.get("category")):
                missing_cat_responses.append(
                    {
                        "responses_file": str(resp_path),
                        "question_id": qid,
                        "model": mr.get("model"),
                        "response_line": resp_line.get(qid),
                    }
                )
        for qid, ca in ana_by_qid.items():
            if _is_missing_category(ca.get("category")):
                missing_cat_analyses.append(
                    {
                        "analysis_file": str(analyses_path),
                        "question_id": qid,
                        "model": ca.get("model"),
                        "analysis_line": ana_line.get(qid),
                    }
                )

        # Mismatches: analysis.response vs responses.response for the same qid
        for qid, ca in ana_by_qid.items():
            mr = resp_by_qid.get(qid)
            if mr is None:
                continue  # out-of-scope: orphan analysis
            # Staleness first (helpful for triage)
            if is_stale(mr.get("timestamp"), ca.get("timestamp")):
                stale_analyses.append(
                    {
                        "responses_file": str(resp_path),
                        "analysis_file": str(analyses_path),
                        "question_id": qid,
                        "model": mr.get("model") or ca.get("model"),
                        "response_ts": mr.get("timestamp"),
                        "analysis_ts": ca.get("timestamp"),
                        "response_line": resp_line.get(qid),
                        "analysis_line": ana_line.get(qid),
                    }
                )

            if not compare_content_bearing(ca.get("response"), mr.get("response")):
                response_mismatches.append(
                    {
                        "responses_file": str(resp_path),
                        "analysis_file": str(analyses_path),
                        "question_id": qid,
                        "model": mr.get("model") or ca.get("model"),
                        "response_line": resp_line.get(qid),
                        "analysis_line": ana_line.get(qid),
                    }
                )
    return (
        files_to_rejudge,
        response_mismatches,
        stale_analyses,
        missing_cat_responses,
        missing_cat_analyses,
    )


def build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="Check compliance correctness across responses and analyses.")
    p.add_argument("--responses-dir", type=Path, default=Path("responses"))
    p.add_argument("--analysis-dir", type=Path, default=Path("analysis"))
    p.add_argument("--json", action="store_true", help="Print machine-readable JSON output")
    p.add_argument("--fix", action="store_true", help="Delete bad analysis rows and rerun judge for affected files")
    # Pass-through options to judge_compliance when using --fix
    p.add_argument("--workers", type=int, default=None, help="Workers to pass to judge_compliance (fix mode)")
    p.add_argument("--max-errors", type=int, default=None, help="Max errors to pass to judge_compliance (fix mode)")
    p.add_argument("--force-restart", action="store_true", help="Force judge_compliance to start fresh (fix mode)")
    return p


def _rewrite_analysis_without_qids(analysis_path: Path, remove_qids: List[str]) -> int:
    """Rewrite analysis file dropping any rows with question_id in remove_qids.

    Returns number of rows removed. Always creates a .bak backup first.
    """
    if not analysis_path.exists() or not remove_qids:
        return 0
    remove_set = set(remove_qids)
    backup = analysis_path.with_suffix(analysis_path.suffix + ".bak")
    try:
        shutil.copy2(analysis_path, backup)
    except Exception:
        # Best-effort backup; continue even if it fails
        pass

    kept_lines: List[str] = []
    removed = 0
    try:
        with analysis_path.open("r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if not s:
                    continue
                try:
                    obj = json.loads(s)
                except json.JSONDecodeError:
                    # Preserve unparseable lines verbatim to avoid data loss
                    kept_lines.append(line)
                    continue
                qid = obj.get("question_id")
                if qid in remove_set:
                    removed += 1
                    continue
                kept_lines.append(json.dumps(obj, ensure_ascii=False) + "\n")
    except FileNotFoundError:
        return 0

    with analysis_path.open("w", encoding="utf-8") as out:
        for ln in kept_lines:
            out.write(ln)
    return removed


def _run_judge_for_files(response_files: List[Path], workers: int | None, max_errors: int | None, force_restart: bool) -> int:
    """Invoke judge_compliance.py for the provided response files.

    Returns process return code (0 when all successful, first non-zero otherwise).
    """
    if not response_files:
        return 0
    cmd = [sys.executable, str(repo_root / "judge_compliance.py")]
    if workers is not None:
        cmd += ["--workers", str(workers)]
    if max_errors is not None:
        cmd += ["--max-errors", str(max_errors)]
    if force_restart:
        cmd.append("--force-restart")
    # Append paths
    cmd += [str(p) for p in response_files]

    print("Running:", " ".join(cmd))
    try:
        proc = subprocess.run(cmd, check=False)
        return proc.returncode
    except Exception as exc:
        print("Failed to run judge_compliance:", exc)
        return 1


def _find_questions_file_for_responses(responses_path: Path, questions_dir: Path = repo_root / "questions") -> Path | None:
    """Best-effort mapping from a responses file to its upstream questions file.

    Strategy: choose the questions/*.jsonl whose stem is a prefix of responses_path.stem
    followed by an underscore, or equals the entire stem. If multiple match, take
    the longest stem (most specific).
    """
    if not questions_dir.exists():
        return None
    rstem = responses_path.stem
    best: tuple[int, Path] | None = None
    for q in questions_dir.glob("*.jsonl"):
        qstem = q.stem
        if rstem == qstem or rstem.startswith(qstem + "_"):
            cand = (len(qstem), q)
            if best is None or cand[0] > best[0]:
                best = cand
    return best[1] if best else None


def _load_questions_category_map(qfile: Path) -> tuple[dict[str, str], str]:
    """Load mapping question_id -> category from questions file. Returns (map, fallback_category)."""
    mapping: dict[str, str] = {}
    fallback = qfile.stem
    try:
        with qfile.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    continue
                if not isinstance(obj, dict):
                    continue
                qid = obj.get("id")
                if isinstance(qid, str):
                    cat = obj.get("category")
                    if isinstance(cat, str) and cat.strip():
                        mapping[qid] = cat
    except FileNotFoundError:
        pass
    return mapping, fallback


def _rewrite_jsonl_fill_category(file_path: Path, qids: List[str], cat_map: dict[str, str], fallback_category: str | None) -> tuple[int, int]:
    """Fill missing/undefined category for the given qids in a JSONL file.

    Returns (updated_count, requested_count). Creates a .bak backup first.
    """
    if not file_path.exists() or not qids:
        return 0, 0
    to_fix = set(qids)
    backup = file_path.with_suffix(file_path.suffix + ".bak")
    try:
        shutil.copy2(file_path, backup)
    except Exception:
        pass

    updated = 0
    out_lines: List[str] = []
    try:
        with file_path.open("r", encoding="utf-8") as f:
            for raw in f:
                s = raw.strip()
                if not s:
                    continue
                try:
                    obj = json.loads(s)
                except json.JSONDecodeError:
                    out_lines.append(raw)
                    continue
                if not isinstance(obj, dict):
                    out_lines.append(json.dumps(obj, ensure_ascii=False) + "\n")
                    continue
                qid = obj.get("question_id") or obj.get("id")
                if isinstance(qid, str) and qid in to_fix:
                    cat_val = obj.get("category")
                    if cat_val is None or (isinstance(cat_val, str) and cat_val.strip().lower() in {"", "undefined"}):
                        new_cat = cat_map.get(qid) or fallback_category
                        if isinstance(new_cat, str) and new_cat:
                            obj["category"] = new_cat
                            updated += 1
                out_lines.append(json.dumps(obj, ensure_ascii=False) + "\n")
    except FileNotFoundError:
        return 0, len(qids)

    with file_path.open("w", encoding="utf-8") as out:
        for ln in out_lines:
            out.write(ln)

    return updated, len(qids)


def main() -> None:
    args = build_arg_parser().parse_args()

    (
        files_to_rejudge,
        mismatches,
        stale,
        missing_cat_responses,
        missing_cat_analyses,
    ) = scan(args.responses_dir, args.analysis_dir)

    if args.json:
        print(
            json.dumps(
                {
                    "missing_responses": [str(p) for p in files_to_rejudge],
                    "response_mismatches": mismatches,
                    "stale_analyses": stale,
                    "missing_category_responses": missing_cat_responses,
                    "missing_category_analyses": missing_cat_analyses,
                    "summary": {
                        "missing_responses": len(files_to_rejudge),
                        "response_mismatches": len(mismatches),
                        "stale_analyses": len(stale),
                        "missing_category_responses": len(missing_cat_responses),
                        "missing_category_analyses": len(missing_cat_analyses),
                    },
                },
                ensure_ascii=False,
            )
        )
        return

    # Default: dry-run reporting
    # If --fix provided, we will attempt to clean and re-judge after reporting.

    print("=== Correctness Check ===")
    print("\nMissing responses (rejudge these files):")
    if files_to_rejudge:
        for p in files_to_rejudge:
            print(f"- {p}")
    else:
        print("- None")

    print("\nResponse mismatches (content-level):")
    if mismatches:
        print(f"count={len(mismatches)}")
        for m in mismatches:
            print(
                f"- file={m['responses_file']} model={m.get('model')} qid={m['question_id']}"
                + (f" (resp line {m['response_line']}, analysis line {m['analysis_line']})" if m.get("response_line") or m.get("analysis_line") else "")
            )
    else:
        print("- None")

    print("\nStale analyses (analysis older than response):")
    if stale:
        print(f"count={len(stale)}")
        for s in stale:
            print(
                f"- file={s['responses_file']} model={s.get('model')} qid={s['question_id']}"
                + (f" (resp ts {s.get('response_ts')}, analysis ts {s.get('analysis_ts')})" if s.get("response_ts") or s.get("analysis_ts") else "")
            )
    else:
        print("- None")

    print("\nMissing category in responses:")
    if missing_cat_responses:
        print(f"count={len(missing_cat_responses)}")
        for r in missing_cat_responses:
            print(
                f"- file={r['responses_file']} model={r.get('model')} qid={r['question_id']}"
                + (f" (resp line {r.get('response_line')})" if r.get("response_line") else "")
            )
    else:
        print("- None")

    print("\nMissing category in analyses:")
    if missing_cat_analyses:
        print(f"count={len(missing_cat_analyses)}")
        for a in missing_cat_analyses:
            print(
                f"- file={a['analysis_file']} model={a.get('model')} qid={a['question_id']}"
                + (f" (analysis line {a.get('analysis_line')})" if a.get("analysis_line") else "")
            )
    else:
        print("- None")

    if not args.fix:
        return

    # --- Fix mode: repair metadata, then remove bad rows and re-run judge ---
    print("\nEntering --fix mode: fixing missing categories, removing bad rows, and rejudging as needed…")

    # 1) Fix missing categories in responses and analyses by consulting questions files
    resp_fix_groups: Dict[Path, set[str]] = defaultdict(set)
    for r in missing_cat_responses:
        resp_fix_groups[Path(r["responses_file"])].add(r["question_id"])
    ana_fix_groups: Dict[Path, set[str]] = defaultdict(set)
    for a in missing_cat_analyses:
        ana_fix_groups[Path(a["analysis_file"])].add(a["question_id"])

    # Cache of questions file -> (cat_map, fallback)
    qfile_cache: Dict[Path, tuple[dict[str, str], str]] = {}

    # Helper to fill a file
    def _fill_file_categories(file_path: Path, qids: List[str], via_responses_path: Path) -> tuple[int, int]:
        qfile = _find_questions_file_for_responses(via_responses_path)
        if not qfile:
            # Cannot locate questions; skip gracefully
            return 0, len(qids)
        if qfile not in qfile_cache:
            qfile_cache[qfile] = _load_questions_category_map(qfile)
        cat_map, fallback_cat = qfile_cache[qfile]
        return _rewrite_jsonl_fill_category(file_path, qids, cat_map, fallback_cat)

    # Responses: fill directly
    for rpath, qids in resp_fix_groups.items():
        updated, requested = _fill_file_categories(rpath, sorted(qids), via_responses_path=rpath)
        print(f"- Filled categories in {rpath.name}: updated {updated} / requested {requested}")

    # Analyses: map to responses path to find questions, then fill
    for apath, qids in ana_fix_groups.items():
        stem = apath.stem
        if stem.startswith("compliance_"):
            resp_stem = stem[len("compliance_"):]
        else:
            resp_stem = stem
        rpath = repo_root / "responses" / f"{resp_stem}.jsonl"
        updated, requested = _fill_file_categories(apath, sorted(qids), via_responses_path=rpath)
        print(f"- Filled categories in {apath.name}: updated {updated} / requested {requested}")

    # Map analysis file -> qids to remove
    remove_map: Dict[Path, List[str]] = {}
    for m in mismatches:
        ap = Path(m["analysis_file"])
        remove_map.setdefault(ap, []).append(m["question_id"])
    for s in stale:
        ap = Path(s["analysis_file"])
        remove_map.setdefault(ap, []).append(s["question_id"])

    total_removed = 0
    for ap, qids in remove_map.items():
        removed = _rewrite_analysis_without_qids(ap, qids)
        print(f"- Cleaned {ap.name}: removed {removed} rows (requested {len(qids)})")
        total_removed += removed

    # Determine response files to rejudge: any with missing, mismatches, or stale
    affected_responses: Dict[str, Path] = {str(p): p for p in files_to_rejudge}
    for m in mismatches:
        affected_responses.setdefault(m["responses_file"], Path(m["responses_file"]))
    for s in stale:
        affected_responses.setdefault(s["responses_file"], Path(s["responses_file"]))

    if not affected_responses:
        print("No affected response files to rejudge.")
        return

    print(f"Rejudging {len(affected_responses)} response file(s)...")
    rc = _run_judge_for_files(list(affected_responses.values()), args.workers, args.max_errors, args.force_restart)
    if rc != 0:
        print(f"judge_compliance exited with status {rc}")
        sys.exit(rc)
    print("Fix mode completed successfully.")


if __name__ == "__main__":
    main()
